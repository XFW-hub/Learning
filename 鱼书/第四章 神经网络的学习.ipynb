{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36b3b0ef",
   "metadata": {},
   "source": [
    "## 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fafc78",
   "metadata": {},
   "source": [
    "### 均方误差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159d64ef",
   "metadata": {},
   "source": [
    "$\n",
    "E\\,\\,=\\,\\,\\frac{1}{2}\\sum_k{\\left( y_k-t_k \\right) ^2}\n",
    "$\n",
    "（针对单个数据）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64274cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y,t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5995f9b",
   "metadata": {},
   "source": [
    "### 交叉熵损失"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1b7ba5",
   "metadata": {},
   "source": [
    "$\n",
    "E\\,\\,=\\,\\,-\\sum_k{t_k\\log y_k}\n",
    "$\n",
    "（针对单个数据）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e91f5a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y+delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63397da2",
   "metadata": {},
   "source": [
    "### mini-batch学习 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a0d99a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "import numpy as np\n",
    "import gzip\n",
    "import struct\n",
    "# 读取 MNIST 图像文件\n",
    "import os\n",
    "\n",
    "def read_images(file):\n",
    "    with gzip.open(file, 'rb') as f:\n",
    "        # 读取文件头部的元数据（大端字节序）\n",
    "        magic, num_images, rows, cols = struct.unpack('>IIII', f.read(16))\n",
    "        # 读取图像数据\n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8).reshape(num_images, rows * cols)\n",
    "        return images\n",
    "\n",
    "# 读取 MNIST 标签文件\n",
    "def read_labels(file):\n",
    "    with gzip.open(file, 'rb') as f:\n",
    "        # 读取文件头部的元数据（大端字节序）\n",
    "        magic, num_labels = struct.unpack('>II', f.read(8))\n",
    "        # 读取标签数据\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        return labels\n",
    "\n",
    "# 文件路径\n",
    "train_images_file = 'F:/XFW-hub/Learning/train-images-idx3-ubyte.gz'\n",
    "train_labels_file = 'F:/XFW-hub/Learning/train-labels-idx1-ubyte.gz'\n",
    "test_images_file = 'F:/XFW-hub/Learning/t10k-images-idx3-ubyte.gz'\n",
    "test_labels_file = 'F:/XFW-hub/Learning/t10k-labels-idx1-ubyte.gz'\n",
    "\n",
    "# 读取训练和测试数据\n",
    "x_train = read_images(train_images_file)\n",
    "t_train = read_labels(train_labels_file)\n",
    "x_test = read_images(test_images_file)\n",
    "t_test = read_labels(test_labels_file)\n",
    "\n",
    "#独热编码\n",
    "def convert_labels_to_one_hot(labels, num_classes=10):\n",
    "    \n",
    "    # 创建独热编码矩阵\n",
    "    one_hot = np.zeros((len(labels), num_classes))\n",
    "    \n",
    "    # 使用高级索引高效设置值\n",
    "    one_hot[np.arange(len(labels)), labels] = 1.0\n",
    "    \n",
    "    return one_hot\n",
    "t_train = convert_labels_to_one_hot(t_train,10)\n",
    "t_test = convert_labels_to_one_hot(t_test,10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5aa9867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size,batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14df1ef",
   "metadata": {},
   "source": [
    "### mini-batch版交叉熵误差的实现 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c61a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    if y.ndim ==1:\n",
    "        t = t.reshape(1,t.size)\n",
    "        y = y.reshape(1,y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y+1e-7)) / batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40554b70",
   "metadata": {},
   "source": [
    "### 梯度及梯度下降法的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dd6196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f,x):#求梯度\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val  + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "    return grad\n",
    "## 对矩阵求梯度，使用nditer迭代器\n",
    "def gradient_descent(f,init_x,lr=0.01,step_num=100):\n",
    "    x = init_x\n",
    "\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f,x)\n",
    "        x-=lr*grad\n",
    "    return x\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 还原值\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3496157f",
   "metadata": {},
   "source": [
    "### 神经网络的梯度 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b68bbd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)\n",
    "    def predict(self,x):\n",
    "        return np.dot(x,self.W)\n",
    "    def loss(self,x,t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(x)\n",
    "        loss = cross_entropy_error(y,t)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e110f1",
   "metadata": {},
   "source": [
    "## 学习算法的实现        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8391f4",
   "metadata": {},
   "source": [
    "### 随机梯度下降法(SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10117993",
   "metadata": {},
   "source": [
    "step 1:mini-batch\n",
    "\n",
    "step 2:计算梯度\n",
    "\n",
    "step 3:更新参数\n",
    "\n",
    "step 4:重复步骤1，2，3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62614e30",
   "metadata": {},
   "source": [
    "### 两层神经网络的类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e3e285",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class TwolayerNet:\n",
    "    def __init__(self,input_size,hidden_size,output_size,weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std*np.random.randn(input_size,hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hiddden_size,output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    def predict(self,x):\n",
    "        W1,W2 = self.params['W1'],self.params['W2']\n",
    "        b1,b2 = self.params['b1'],self.params['b2']\n",
    "\n",
    "        a1 = np.dot(x,W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1,W2) + b2\n",
    "        y = softmax(a2)\n",
    "\n",
    "        return y \n",
    "    def loss(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y,t)\n",
    "    \n",
    "    def accuracy(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y,axis=1)\n",
    "        t = np.argmax(t,axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t)/float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self,x,t):\n",
    "        loss_W = lambda W:self.loss(x,t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W,self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W,self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W,self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W,self.params['b2'])\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0005b190",
   "metadata": {},
   "source": [
    "### mini-batch的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a7917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwolayerNet(input_size=784,hidden_size=50,output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size,batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grad = network.numerical_gradient(x_batch,t_batch)\n",
    "\n",
    "    for key in ('W1','b1','W2','b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    loss = network.loss(x_batch,t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cac528",
   "metadata": {},
   "source": [
    "### 基于测试数据的评价"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddab958",
   "metadata": {},
   "source": [
    "epoch:所有训练数据均被使用过一次时的更新数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e120aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "iter_per_epoch = max(train_size / batch_size,1)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwolayerNet(input_size=784,hidden_size=50,output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size,batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grad = network.numerical_gradient(x_batch,t_batch)\n",
    "\n",
    "    for key in ('W1','b1','W2','b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    loss = network.loss(x_batch,t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    if i%iter_per_epoch == 0 :\n",
    "        train_acc = network.accuracy(x_train,t_train)\n",
    "        test_acc = network.accuracy(x_test,t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc,test acc |\"+str(train_acc)+\",\"+str(test_acc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
